---
title: "Week 11:</br>Clustering and</br>the Dendrogram"
author: "Visual Data Analytics"
institute: "University of Sydney"
output:
  xaringan::moon_reader:
    chakra: libs/remark-latest.min.js
    lib_dir: libs
    css: [default,"../css/mtheme.css","../css/mod.css"]
    nature:
      slideNumberFormat: "%current%"
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
    includes:
      before_body: ../auxil/defs.html
---

# Outline

- Distance
- Single Linkage
- Other Hierarchical Clustering
- Dendrogram

```{r,include=FALSE}
library(reticulate)
use_python('~/anaconda3/bin/python3')
matplotlib<-import('matplotlib')
matplotlib$use("Agg",force=TRUE)
knitr::opts_chunk$set(fig.align='center',echo=FALSE,message = F,warning = F)
```

---

# Motivation

- We can profile an individual according to their attributes.
  - Are two individuals similar?
  - Can we group to individuals together?
  - How can we visualise this?
- The method is hierarchical clustering and the visualisation is the Dendrogram.
- The ideas we cover are useful in marketing and other business problems.


```{python}
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
```


---

class: middle, center, inverse

# Distance


---


# Why distance?
 
- Many problems that involve thinking about how *similar* or dissimilar two observations are.  For example:<!--D-->
--

  + May use the same marketing strategy for *similar* demographic groups.
  + May lend money to applicants who are *similar* to those who pay debts back.<!--D-->
--

- Arguably the most important concept in data analysis is *distance*

---

# Simple example
 
- Consider 3 individuals:<!--D-->
--

  + Mr Orange: 37 years of age earns $75k a year
  + Mr Red: 31 years of age earns $67k a year
  + Mr Blue: 30 years of age earns $68k a year<!--D-->
--

- Which two are the most similar?

---

# On a scatterplot
 

```{r scatter}
plot(c(37,31,30),c(75,67,68),col=c('orange','red','blue'),pch=20,cex=2,main='Distance',xlab='Age (Years)',ylab='Income ($000)',xlim=c(20,45),ylim=c(55,85))
```

---

# Distance as a number
 
- It is easy to think about three individuals but what if there are thousands of individuals?<!--D-->
--

  + In this case it will be useful to attach some number to the distance between pairs of individuals<!--D-->
--

  + We will do it with a simple application of Pythagoras' theorem.

---
  
# Finding the Distance
 

```{r pytha, fig.show=''}
plot(c(37,31),c(75,67),col='blue',pch=20,cex=2,main='Distance',xlab='Age (Years)',ylab='Income ($000)',xlim=c(20,45),ylim=c(55,85))
```

---

# Finding the Distance
 
  
```{r pythb, fig.show=''}
plot(c(37,31),c(75,67),col='blue',pch=20,cex=2,main='Distance',xlab='Age (Years)',ylab='Income ($000)',xlim=c(20,45),ylim=c(55,85))
text(31,67,'Cust. 1: 31yo earns $67k',pos=2)
```

---

# Finding the Distance
 
  
```{r pythc, fig.show=''}
plot(c(37,31),c(75,67),col='blue',pch=20,cex=2,main='Distance',xlab='Age (Years)',ylab='Income ($000)',xlim=c(20,45),ylim=c(55,85))
text(31,67,'Cust. 1: 31yo earns $67k',pos=2)
text(37,75,'Cust. 2: 37yo earns $75k',pos=4)
```

---

# Finding the Distance
 
  
```{r pythd, fig.show=''}
plot(c(37,31),c(75,67),col='blue',pch=20,cex=2,main='Distance',xlab='Age (Years)',ylab='Income ($000)',xlim=c(20,45),ylim=c(55,85))
text(31,67,'Cust. 1: 31yo earns $67k',pos=2)
text(37,75,'Cust. 2: 37yo earns $75k',pos=4)
lines(c(31,37),c(67,67),lwd=2)
```

---

# Finding the Distance
 
  
```{r pythe, fig.show=''}
plot(c(37,31),c(75,67),col='blue',pch=20,cex=2,main='Distance',xlab='Age (Years)',ylab='Income ($000)',xlim=c(20,45),ylim=c(55,85))
text(31,67,'Cust. 1: 31yo earns $67k',pos=2)
text(37,75,'Cust. 2: 37yo earns $75k',pos=4)
lines(c(31,37),c(67,67),lwd=2)
text(34,67,'Dif=6yrs',pos=1)
```

---

# Finding the Distance
 
  
```{r pythf, fig.show=''}
plot(c(37,31),c(75,67),col='blue',pch=20,cex=2,main='Distance',xlab='Age (Years)',ylab='Income ($000)',xlim=c(20,45),ylim=c(55,85))
text(31,67,'Cust. 1: 31yo earns $67k',pos=2)
text(37,75,'Cust. 2: 37yo earns $75k',pos=4)
lines(c(31,37),c(67,67),lwd=2)
text(34,67,'Dif=6yrs',pos=1)
lines(c(37,37),c(67,75),lwd=2)
```

---

# Finding the Distance
 
  
```{r pythg, fig.show=''}
plot(c(37,31),c(75,67),col='blue',pch=20,cex=2,main='Distance',xlab='Age (Years)',ylab='Income ($000)',xlim=c(20,45),ylim=c(55,85))
text(31,67,'Cust. 1: 31yo earns $67k',pos=2)
text(37,75,'Cust. 2: 37yo earns $75k',pos=4)
lines(c(31,37),c(67,67),lwd=2)
text(34,67,'Dif=6yrs',pos=1)
lines(c(37,37),c(67,75),lwd=2)
text(37,71,'Dif=$8k',pos=4)
```

---

# Finding the Distance
  
```{r pythh, fig.show=''}
plot(c(37,31),c(75,67),col='blue',pch=20,cex=2,main='Distance',xlab='Age (Years)',ylab='Income ($000)',xlim=c(20,45),ylim=c(55,85))
text(31,67,'Cust. 1: 31yo earns $67k',pos=2)
text(37,75,'Cust. 2: 37yo earns $75k',pos=4)
lines(c(31,37),c(67,67),lwd=2)
text(34,67,'Dif=6yrs',pos=1)
lines(c(37,37),c(67,75),lwd=2)
text(37,71,'Dif=$8k',pos=4)
lines(c(31,37),c(67,75),lwd=2)
```

---

# Finding the Distance
 
  
```{r pythi, fig.show=''}
plot(c(37,31),c(75,67),col='blue',pch=20,cex=2,main='Distance',xlab='Age (Years)',ylab='Income ($000)',xlim=c(20,45),ylim=c(55,85))
text(31,67,'Cust. 1: 31yo earns $67k',pos=2)
text(37,75,'Cust. 2: 37yo earns $75k',pos=4)
lines(c(31,37),c(67,67),lwd=2)
text(34,67,'Dif=6yrs',pos=1)
lines(c(37,37),c(67,75),lwd=2)
text(37,71,'Dif=$8k',pos=4)
lines(c(31,37),c(67,75),lwd=2)
text(35,72,'Euclidean Distance is 10  ',pos=2)
```

---

# Euclidean distance
 
- In general there are more than two variables.<!--D-->
--

- Is there a way to apply our intuition in 2 dimensions to higher dimensions?<!--D-->
--

  + Pythagoras' theorem can be *generalised* to higher dimensions.<!--D-->
--

  + This results in a concept of distance called *Euclidean distance*.

---
  
# Euclidean distance
 

We measure $p$ variables for two observations: $x_{j}$ is the measurement of variable $j$ for observation ${\mathbf x}$, $y_{j}$ is the measurement of variable $j$ for observation ${\mathbf y}$.  *Euclidean* distance between ${\mathbf x}$ and ${\mathbf y}$ is:

$$D\left({\mathbf x},{\mathbf y}\right)=\sqrt{\sum\limits_{j=1}^p \left(x_{j}-y_{j}\right)^2}$$

---

# Distance and Standardising data
 
- We must be careful about the units of measurement.<!--D-->
--

- Euclidean distance will change when variables measured in *different units*.<!--D-->
--

- For this reason, it is common to calculate distance after the *standardising* data.<!--D-->
--

- If the variables are all measured in the same units, then this standardisation is unecessary.<!--D-->

---

# Other kinds of distance
 
- We will nearly always use Euclidean Distance in this unit, however there are other ways of understanding distance .
- This includes distance measures for categorical data and even strings of text!
- While we will not cover these, the methods of hierarchical clustering we cover will work as long as we have some way of defining distance between individuals.

---

# Why is distance useful?

```{r}
knitr::include_graphics('img/CollaborativeFiltering.jpeg')
```

Figure by Mohamed Ben Ellefi

---

# Recommender Systems

- Famous recommender systems are used by Amazon, Netflix, Alibaba amongst others.
- These systems are usually a hybrid of 
  - Collaborative Filtering
  - Content-based Filtering
- The method we discussed is more specifically called memory-based collaborative filtering.
- Being able to put customers into similar groups is important.


---


class: middle, center, inverse

# Hierarchical Clustering

---


# Age v Income

```{r,echo=FALSE,fig.align='center'}
library(mvtnorm)
mu1<-c(35,60)
mu2<-c(55,100)
sigma<-matrix(c(25,-30,-30,49),2,2)
x1<-rmvnorm(100,mu1,sigma)
x2<-rmvnorm(50,mu2,sigma)
x<-rbind(x1,x2)
plot(x[,1],x[,2],col='blue',pch=20,cex=3,main='Clusters',xlab='Age (Years)',ylab='Income ($000)')
```

---

# Obvious clusters

```{r,echo=FALSE,fig.align='center'}
plot(x[,1],x[,2],col='blue',pch=20,cex=3,main='Clusters',xlab='Age (Years)',ylab='Income ($000)')
points(x1[,1],x1[,2],col='red',pch=20,cex=3)
```

---

# Summary

- When there are more than 2 variables just looking at a scatterplot doesnâ€™t work.<!--D-->
--

- Instead algorithms can be used to find the clusters in a sensible way, even in high dimensions.

```{r,include=FALSE,echo=FALSE, message=FALSE}
library(tibble)
library(magrittr)
```

---

# Definition of Clustering

- Oxford Dictionary: A group of similar things or people positioned or occurring closely together<!--D-->
--

- Collins Dictionary: A number of things growing, fastened, or occurring close together<!--D-->
--

- Note the importance of closeness or distance.  We need two concepts of distance<!--D-->
--

  1. Distance between **observations**.
  2. Distance between **clusters**.

---

# A distance between clusters

- Let $\mathcal{A}$ be a cluster with observations $\left\{{\mathbf a}_1, {\mathbf a}_2, \ldots, {\mathbf a}_I \right\}$ and $\mathcal{B}$ be a cluster with points $\left\{{\mathbf b}_1, {\mathbf b}_2, \ldots, {\mathbf b}_J \right\}$.
--

- The calligraphic script $\mathcal{A}$ or $\mathcal{B}$ denotes a cluster with possibly more than one point.  
--

- The bold script ${\mathbf a}_i$ or ${\mathbf b}_j$ denotes a vector of attributes (e.g. age and income) for each observation.
--

- Rather than vectors, it is much easier to think of each observation as a point in a scatterplot. 

---

#Single Linkage

One way of defining the distance between clusters $\mathcal{A}$ and $\mathcal{B}$ is

$$D(\mathcal{A},\mathcal{B})=\underset{i,j}{\min}D({\mathbf a}_i,{\mathbf b}_j)$$

This is called **single linkage** or **nearest neighbour**.

---

# Single Linkage

```{r slinkp,echo=FALSE,fig.align='center'}
set.seed(4)
mu1<-c(15,30)
mu2<-c(85,120)
sigma<-matrix(c(25,30,30,49),2,2)
x1<-rmvt(30,df=5,sigma)+mu1
x2<-rmvnorm(25,mu2,sigma)
x<-rbind(x1,x2)
plot(x[,1],x[,2],col='blue',pch=20,cex=3,main='Clusters',xlab='',ylab='')
```

---

# Single Linkage


```{r slink,echo=FALSE,fig.align='center'}
plot(x[,1],x[,2],col='blue',pch=20,cex=3,main='Clusters',xlab='',ylab='')
points(x2[,1],x2[,2],col='red',pch=20,cex=3)
lines(c(x1[12,1],x2[6,1]),c(x1[12,2],x2[6,2]),lwd=4)
text(mean(c(x1[12,1],x2[6,1])),mean(c(x1[12,2],x2[6,2])),labels='68.83',pos=4,cex=3)
```

---


# Complete Linkage

Another way of defining the distance between $\mathcal{A}$ and $\mathcal{B}$ is

$$D(\mathcal{A},\mathcal{B})=\underset{i,j}{\max}D({\mathbf a}_i,{\mathbf b}_j)$$

This is called **complete linkage** or **furthest neighbour**.


---

# Complete Linkage

```{r clinkp,echo=FALSE,fig.align='center'}
set.seed(4)
mu1<-c(15,30)
mu2<-c(85,120)
sigma<-matrix(c(25,30,30,49),2,2)
x1<-rmvt(30,df=5,sigma)+mu1
x2<-rmvnorm(25,mu2,sigma)
x<-rbind(x1,x2)
plot(x[,1],x[,2],col='blue',pch=20,cex=3,main='Clusters',xlab='',ylab='')
```

---

# Complete Linkage


```{r clink,echo=FALSE,fig.align='center'}
plot(x[,1],x[,2],col='blue',pch=20,cex=3,main='Clusters',xlab='',ylab='')
points(x2[,1],x2[,2],col='red',pch=20,cex=3)
lines(c(x1[23,1],x2[4,1]),c(x1[23,2],x2[4,2]),lwd=4)
text(mean(c(x1[23,1],x2[4,1])),mean(c(x1[23,2],x2[4,2])),labels='160.01',pos=4,cex=3)
```

---

# Complete linkage

- In the previous example **all** points in the red cluster are within a distance of 160.01 of **all** points in the blue cluster.
- This is why it is called **complete** linkage.


---

# A simple example

- Over the next couple of slides we will go through the entire process of agglomerative clustering<!--D-->
--

  + We will use Euclidean distance to define distance between points<!--D-->
--

  + We will use single linkage to define the distance between clusters<!--D-->
--

- There are only five observations and two variables

---

# Agglomerative clustering

```{r,echo=FALSE,fig.align='center',message=FALSE,warning=FALSE}
library(xtable)
library(plotrix)
m<-2
n<-5
set.seed(5)
x<-matrix(runif(n*m),n,m)
lets<-LETTERS[seq(1,n,)]
row.names(x)<-lets
dd<-dist(x,diag=TRUE)
cols<-rainbow(5)
par(bg='gray')
plot(x[,1],x[,2],col=cols,main="Cluster Analysis",pch=18,cex=3,xlim=c(-0,1),ylim=c(-0,1),xlab='',ylab='')
text(x[,1],x[,2],labels=lets,pos=1,col=cols,cex=3)
```

---

# Agglomerative clustering


```{r,echo=FALSE,fig.align='center'}
par(bg='gray')
plot(x[,1],x[,2],col=cols,main="Cluster Analysis",pch=18,cex=3,xlim=c(-0,1),ylim=c(-0,1),xlab='',ylab='')
text(x[,1],x[,2],labels=lets,pos=1,col=cols,cex=3)
lines(c(x[1,1],x[4,1]),c(x[1,2],x[4,2]),lwd=3)
#points(x[4,1],x[4,2],col=cols[1],pch=18,cex=1)
#text(x[4,1],x[4,2],labels=lets[4],pos=1,col=cols[1],cex=1)
```

---

# Agglomerative clustering


```{r,echo=FALSE,fig.align='center'}
par(bg='gray')
plot(x[,1],x[,2],col=cols,main="Cluster Analysis",pch=18,cex=3,xlim=c(-0,1),ylim=c(-0,1),xlab='',ylab='')
text(x[,1],x[,2],labels=lets,pos=1,col=cols,cex=3)
lines(c(x[1,1],x[4,1]),c(x[1,2],x[4,2]),lwd=3)
points(x[4,1],x[4,2],col=cols[1],pch=18,cex=3)
text(x[4,1],x[4,2],labels=lets[4],pos=1,col=cols[1],cex=3)
```

---

# Agglomerative clustering


```{r,echo=FALSE,fig.align='center'}
par(bg='gray')
plot(x[,1],x[,2],col=cols,main="Cluster Analysis",pch=18,cex=3,xlim=c(-0,1),ylim=c(-0,1),xlab='',ylab='')
text(x[,1],x[,2],labels=lets,pos=1,col=cols,cex=3)
points(x[4,1],x[4,2],col=cols[1],pch=18,cex=3)
text(x[4,1],x[4,2],labels=lets[4],pos=1,col=cols[1],cex=3)
draw.circle(mean(c(x[1,1],x[4,1])),mean(c(x[1,2],x[4,2])),radius=dd[3]/1.9,border=cols[1],lwd=3)
```

---

# Agglomerative clustering


```{r,echo=FALSE,fig.align='center'}
par(bg='gray')
plot(x[,1],x[,2],col=cols,main="Cluster Analysis",pch=18,cex=3,xlim=c(-0,1),ylim=c(-0,1),xlab='',ylab='')
text(x[,1],x[,2],labels=lets,pos=1,col=cols,cex=3)
points(x[4,1],x[4,2],col=cols[1],pch=18,cex=3)
text(x[4,1],x[4,2],labels=lets[4],pos=1,col=cols[1],cex=3)
draw.circle(mean(c(x[1,1],x[4,1])),mean(c(x[1,2],x[4,2])),radius=dd[3]/1.9,border=cols[1],lwd=3)
lines(c(x[2,1],x[3,1]),c(x[2,2],x[3,2]),lwd=3)
#points(x[2,1],x[2,2],col=cols[3],pch=18,cex=1)
#text(x[2,1],x[2,2],labels=lets[4],pos=1,col=cols[3],cex=1)
#draw.circle(mean(c(x[2,1],x[3,1])),mean(c(x[2,2],x[3,2])),radius=dd[5]/1.9,border=cols[3],lwd=3)
```

---

# Agglomerative clustering


```{r,echo=FALSE,fig.align='center'}
par(bg='gray')
plot(x[,1],x[,2],col=cols,main="Cluster Analysis",pch=18,cex=3,xlim=c(-0,1),ylim=c(-0,1),xlab='',ylab='')
text(x[,1],x[,2],labels=lets,pos=1,col=cols,cex=3)
points(x[4,1],x[4,2],col=cols[1],pch=18,cex=3)
text(x[4,1],x[4,2],labels=lets[4],pos=1,col=cols[1],cex=3)
draw.circle(mean(c(x[1,1],x[4,1])),mean(c(x[1,2],x[4,2])),radius=dd[3]/1.9,border=cols[1],lwd=3)
lines(c(x[2,1],x[3,1]),c(x[2,2],x[3,2]),lwd=3)
points(x[2,1],x[2,2],col=cols[3],pch=18,cex=3)
text(x[2,1],x[2,2],labels=lets[2],pos=1,col=cols[3],cex=3)
#draw.circle(mean(c(x[2,1],x[3,1])),mean(c(x[2,2],x[3,2])),radius=dd[5]/1.9,border=cols[3],lwd=3)
```

---

# Agglomerative clustering


```{r,echo=FALSE,fig.align='center'}
par(bg='gray')
plot(x[,1],x[,2],col=cols,main="Cluster Analysis",pch=18,cex=3,xlim=c(-0,1),ylim=c(-0,1),xlab='',ylab='')
text(x[,1],x[,2],labels=lets,pos=1,col=cols,cex=3)
points(x[4,1],x[4,2],col=cols[1],pch=18,cex=3)
text(x[4,1],x[4,2],labels=lets[4],pos=1,col=cols[1],cex=3)
draw.circle(mean(c(x[1,1],x[4,1])),mean(c(x[1,2],x[4,2])),radius=dd[3]/1.9,border=cols[1],lwd=3)
#lines(c(x[2,1],x[3,1]),c(x[2,2],x[3,2]),lwd=3)
points(x[2,1],x[2,2],col=cols[3],pch=18,cex=3)
text(x[2,1],x[2,2],labels=lets[2],pos=1,col=cols[3],cex=3)
draw.circle(mean(c(x[2,1],x[3,1])),mean(c(x[2,2],x[3,2])),radius=dd[5]/1.7,border=cols[3],lwd=3)
```

---

# Agglomerative clustering


```{r,echo=FALSE,fig.align='center'}
par(bg='gray')
plot(x[,1],x[,2],col=cols,main="Cluster Analysis",pch=18,cex=3,xlim=c(-0,1),ylim=c(-0,1),xlab='',ylab='')
text(x[,1],x[,2],labels=lets,pos=1,col=cols,cex=3)
points(x[4,1],x[4,2],col=cols[1],pch=18,cex=3)
text(x[4,1],x[4,2],labels=lets[4],pos=1,col=cols[1],cex=3)
draw.circle(mean(c(x[1,1],x[4,1])),mean(c(x[1,2],x[4,2])),radius=dd[3]/1.9,border=cols[1],lwd=3)
#lines(c(x[2,1],x[3,1]),c(x[2,2],x[3,2]),lwd=3)
points(x[2,1],x[2,2],col=cols[1],pch=18,cex=3)
text(x[2,1],x[2,2],labels=lets[2],pos=1,col=cols[1],cex=3)
points(x[3,1],x[3,2],col=cols[1],pch=18,cex=3)
text(x[3,1],x[3,2],labels=lets[3],pos=1,col=cols[1],cex=3)
draw.circle(mean(c(x[2,1],x[3,1])),mean(c(x[2,2],x[3,2])),radius=dd[5]/1.7,border=cols[1],lwd=3)
```

---

# Agglomerative clustering


```{r,echo=FALSE,fig.align='center'}
par(bg='gray')
plot(x[,1],x[,2],col=cols,main="Cluster Analysis",pch=18,cex=3,xlim=c(-0,1),ylim=c(-0,1),xlab='',ylab='')
text(x[,1],x[,2],labels=lets,pos=1,col=cols,cex=3)
points(x[4,1],x[4,2],col=cols[1],pch=18,cex=3)
text(x[4,1],x[4,2],labels=lets[4],pos=1,col=cols[1],cex=3)
#draw.circle(mean(c(x[1,1],x[4,1])),mean(c(x[1,2],x[4,2])),radius=dd[3]/1.9,border=cols[1],lwd=3)
#lines(c(x[2,1],x[3,1]),c(x[2,2],x[3,2]),lwd=3)
points(x[2,1],x[2,2],col=cols[1],pch=18,cex=3)
text(x[2,1],x[2,2],labels=lets[2],pos=1,col=cols[1],cex=3)
points(x[3,1],x[3,2],col=cols[1],pch=18,cex=3)
text(x[3,1],x[3,2],labels=lets[3],pos=1,col=cols[1],cex=3)
draw.circle(mean(x[1:4,1]),mean(x[1:4,2]),radius=0.45,border=cols[1],lwd=3)
```

---

# Agglomerative clustering


```{r,echo=FALSE,fig.align='center'}
par(bg='gray')
plot(x[,1],x[,2],col=cols[1],main="Cluster Analysis",pch=18,cex=3,xlim=c(-0,1),ylim=c(-0,1),xlab='',ylab='')
text(x[,1],x[,2],labels=lets,pos=1,col=cols[1],cex=3)
points(x[4,1],x[4,2],col=cols[1],pch=18,cex=3)
text(x[4,1],x[4,2],labels=lets[4],pos=1,col=cols[1],cex=3)
```

---

# Hierarchical Clustering

- 5-cluster solution A and B and C and D and E
--

- 4-cluster solution \{A,D\} and B and C and E  
--

- 3-cluster solution \{A,D\} and \{B, C\} and E
--

- 2-cluster solution \{A,B, C,D\} and E
--

- 1-cluster solution \{A,B, C,D E\}

---

# Dendrogram

- The Dendrogram is a useful tool for analysing a cluster solution.<!--D-->
--

  + Observations are on one axis (usually x)<!--D-->
--

  + The distance between clusters is on other axis (usually y).<!--D-->
--

  + From the Dendrogram one can see the order in which the clusters are merged.

---

# Dendrogram

```{r,echo=FALSE,fig.align='center'}
sl<-hclust(dist(x),method = 'single')
plot(sl,cex=3)
```

---

# Interpretation of Dendrogram

- Think of the axis with distance (y-axis) as the measuring a 'tolerance level'<!--D-->
--

- If the distance between two clusters is within the tolerance they are merged into one cluster.<!--D-->
--

- As tolerance increases more and more clusters are merged leading to less clusters overall.<!--D-->

---

# A real example using Python

- We will use the mpg dataset from Seaborn
  - Observations are cars
  - Variables are related to engine size, fuel efficiency, etc.
- Will make car name the index
- Will remove non numeric variables (origin and name)
- We will drop observations with missing values.

---

# Data processing

```{python, echo=TRUE}
cars =  sns.load_dataset('mpg')
cars73 = cars[cars['model_year']==73]
cars73.index = cars73['name']
carsnum = cars73.iloc[:,0:6]
carsnum = carsnum.dropna(how = 'any')
carsnum
```

---
# Plot

```{python, echo=TRUE,out.width='60%'}
sns.clustermap(carsnum, standard_scale=1)


```

---

# What do we see?

- Notice there are two dendrograms
  - One groups observations together
  - The other groups variables together
- The inside is a heatmap for the data matrix
- Cars most easily grouped by cylinders.
- Also groupings in variables.

---

# Dendrogram only

```{python, echo=TRUE,out.width='70%'}
from scipy.cluster.hierarchy import dendrogram, linkage
import numpy as np
plt.figure()
Z = linkage(carsnum, 'average')
dendrogram(Z, orientation = 'right', leaf_font_size=8, labels=carsnum.index)
```

---

# Dendrogram

```{python, echo=TRUE,out.width='60%'}
plt.show()
```

---

# More about the code

- The hierarchical clustering is done using the scipy package.
- Information can also be pulled out of the object created by this package.
- By default this package does not do simple linkage.
- We will now see why.


---

class: middle, center, inverse

# Other clustering methods



---

# Pros and Cons of Single Linkage

- Pros:
  + Single linkage is very easy to understand.
  + Single linkage is a very fast algorithm.<!--D-->
--

- Cons:
  + Single linkage is very sensitive to single observations which leads to chaining.
  + Complete linkage avoids this problem and gives more compact clusters with a similar diameter.



---

# Chaining

```{r,echo=FALSE,eval=TRUE,fig.align='center', warning=FALSE}
RNGversion("2.15.2")
set.seed(7)
library(mvtnorm)
library(stats)
mu1<-c(5,10)
mu2<-c(10,5)
mu3<-c(10,10)
Sigma<-matrix(c(0.5,0.1,0.1,0.5),2,2)
x1<-rmvt(15,df=10,Sigma)+t(matrix(rep(mu1,15),2,15))
x2<-rmvt(20,df=8,Sigma)+t(matrix(rep(mu2,20),2,20))
x3<-rmvt(10,df=5,Sigma)+t(matrix(rep(mu3,10),2,10))
x<-rbind(x1,x2,x3)
xm<-x[c(1:45),]
par(bg='gray')
plot(xm[,1],xm[,2],pch=19,cex=3,main="Data",xlab='',ylab='',asp = 1)
```

---

# Single Linkage Dendrogram

```{r,echo=FALSE,eval=TRUE,fig.align='center'}
ddm<-dist(xm)
sls<-hclust(ddm,method = "single")
plot(sls,hang=-1,xlab='',sub='')
```

---

# Single Linkage

```{r,echo=FALSE,eval=TRUE,fig.align='center'}
inds<-cutree(sls,k=3)
par(bg='gray')
plot(xm[,1],xm[,2],pch=19,cex=3,main="3-cluster solution",xlab='',ylab='',asp = 1)
points(xm[(inds==1),1],xm[(inds==1),2],pch=19,cex=3,col='red')
points(xm[(inds==2),1],xm[(inds==2),2],pch=19,cex=3,col='blue')
points(xm[(inds==3),1],xm[(inds==3),2],pch=19,cex=3,col='green')
```

---

# Add one observation


```{r,echo=FALSE,eval=TRUE,fig.align='center'}
x<-rbind(x,c(10,7.5))
dd<-dist(x)
sls<-hclust(dd,method = "single")
par(bg='gray')
plot(x[,1],x[,2],pch=19,cex=3,main="Data",xlab='',ylab='',asp = 1)
points(x[46,1],x[46,2],pch=19,cex=3,col='magenta')
```

---

# New solution

```{r,echo=FALSE,eval=TRUE,fig.align='center'}
inds<-cutree(sls,k=3)
par(bg='gray')
plot(x[,1],x[,2],pch=18,main="3-cluster solution",xlab='',ylab='',asp = 1)
points(x[(inds==1),1],x[(inds==1),2],pch=19,col='red',cex=3)
points(x[(inds==2),1],x[(inds==2),2],pch=19,col='blue',cex=3)
points(x[(inds==3),1],x[(inds==3),2],pch=19,col='green',cex=3)
```

---

# Dendrogram with Chaining

```{r,echo=FALSE,eval=TRUE,fig.align='center'}
plot(sls,hang=-1,xlab='',sub='')
```

---

# Robustness

- In general adding a single observation should not dramatically change the analysis.
--

- In this instance the new observation was not even an *outlier*.
--

- A term used for such an observation is an *inlier*.
--

- Methods that are not affected by single observations are often called **robust**.
--

- Let's see if complete linkage is *robust* to the inlier.

---

# Complete Linkage

```{r,echo=FALSE,eval=TRUE,fig.align='center'}
cls<-hclust(dd,method = "complete")
indc<-cutree(cls,k=3)
par(bg='gray')
plot(x[,1],x[,2],pch=19,cex=3,main="3-cluster solution",xlab='',ylab='',asp = 1)
points(x[(indc==1),1],x[(indc==1),2],pch=19,cex=3,col='red')
points(x[(indc==2),1],x[(indc==2),2],pch=19,cex=3,col='blue')
points(x[(indc==3),1],x[(indc==3),2],pch=19,cex=3,col='green')
```

---

# Complete Linkage: Dendrogram

```{r,echo=FALSE,eval=TRUE,fig.align='center'}
plot(cls,hang=-1,xlab='',sub='')
```

---

# Disadvantages of CL

- Complete Linkage overcomes *chaining* and is robust to inliers
--

- However, since the distance between clusters only depends on two observations it can still be sensitive to outliers.<!--D-->
--

- The following methods are more robust and should be preferred<!--D-->
--

  + Average Linkage
  + Centroid Method
  + Wardâ€™s Method

---

# Average Linkage

The distance between two clusters can be defined so that it is based on all the pairwise distances between the elements of each cluster.
$$D(\mathcal{A},\mathcal{B})=\frac{1}{|\mathcal{A}||\mathcal{B}|}\sum\limits_{i=1}^{|\mathcal{A}|}\sum\limits_{j=1}^{|\mathcal{B}|}D({\mathbf a}_i,{\mathbf b}_j)$$
Here $|\mathcal{A}|$ is the number of observations in cluster $\mathcal{A}$ and $|\mathcal{B}|$ is the number of observations in cluster $\mathcal{B}$

---

#Average Linkage

- Average linkage can be called different things<!--D-->
--

  + Between groups method.
  + Unweighted Pair Group Method with Arithmetic mean (UPGMA)

---

# Pairwise distances (one obs.)

```{r,echo=FALSE,fig.align='center'}
set.seed(4)
mu1<-c(15,30)
mu2<-c(85,120)
sigma<-matrix(c(25,30,30,49),2,2)
x1<-rmvt(30,df=5,sigma)+mu1
x2<-rmvnorm(25,mu2,sigma)
x<-rbind(x1,x2)
plot(x[21:35,1],x[21:35,2],col='blue',pch=20,cex=3,main='Clusters',xlab='',ylab='')
points(x2[1:5,1],x2[1:5,2],col='red',pch=20,cex=3)
for (i in 1:5){
lines(c(x1[21,1],x2[i,1]),c(x1[21,2],x2[i,2]),lwd=2.5)
}
```

---

# All pairwise distances

```{r,echo=FALSE,fig.align='center'}
plot(x[21:35,1],x[21:35,2],col='blue',pch=20,cex=3,main='Clusters',xlab='',ylab='')
points(x2[1:5,1],x2[1:5,2],col='red',pch=20,cex=3)
for (i in 1:5){
  for (j in 21:30){
    lines(c(x1[j,1],x2[i,1]),c(x1[j,2],x2[i,2]),lwd=0.5)
  }
}
```

---

# Centroid Method

- The centroid of a cluster can be defined as the mean of all the
points in the cluster.<!--D-->
--

- If $\mathcal{A}$ is a cluster containing the observations ${\mathbf a}$ then the **centroid** of $\mathcal{A}$ is given by.<!--D-->
--

$${\mathbf{\bar{a}}}=\frac{1}{|\mathcal{A}|}\sum_{\mathbf{a}_i\in\mathcal{A}}\mathbf{a}_i$$<!--D-->
--

- The distance between two clusters can then be defined as the distance between the respective centroids.

---

# Vector mean

- Recall that $\mathbf{a}_i$ is a vector of attributes, e.g income and age.
--

- In this case $\bar{\mathbf{a}}$ is also a vector of attributes.
--

- Each element of $\bar{\mathbf{a}}$ is the mean of a different attribute, e.g. mean income, mean age.

---

# Centroid method

```{r,echo=FALSE,fig.align='center', warning=FALSE, message=FALSE}
RNGversion("2.15.2")
set.seed(7)
library(mvtnorm)
library(stats)
mu1<-c(5,10)
mu2<-c(10,5)
mu3<-c(10,10)
Sigma<-matrix(c(0.5,0.1,0.1,0.5),2,2)
x1<-rmvt(15,df=10,Sigma)+t(matrix(rep(mu1,15),2,15))
x2<-rmvt(20,df=8,Sigma)+t(matrix(rep(mu2,20),2,20))
x3<-rmvt(10,df=5,Sigma)+t(matrix(rep(mu3,10),2,10))
x<-rbind(x1,x2,x3)
dd<-dist(x)
cls<-hclust(dd,method = "complete")
indc<-cutree(cls,k=3)
par(bg='gray')
plot(x[,1],x[,2],pch=18,main="3-cluster solution",xlab='',ylab='',asp = 1)
points(x[(indc==1),1],x[(indc==1),2],pch=18,col='red',cex=3)
points(x[(indc==2),1],x[(indc==2),2],pch=18,col='blue',cex=3)
points(x[(indc==3),1],x[(indc==3),2],pch=18,col='green',cex=3)
```

---

# Centroid method

```{r,echo=FALSE,fig.align='center'}
par(bg='gray')
plot(x[,1],x[,2],pch=18,main="3-cluster solution",xlab='',ylab='',asp = 1)
points(x[(indc==1),1],x[(indc==1),2],pch=18,col='red',cex=3)
points(x[(indc==2),1],x[(indc==2),2],pch=18,col='blue',cex=3)
points(x[(indc==3),1],x[(indc==3),2],pch=18,col='green',cex=3)
points(mean(x[(indc==1),1]),mean(x[(indc==1),2]),pch=10,col='black',cex=5)
points(mean(x[(indc==2),1]),mean(x[(indc==2),2]),pch=10,col='black',cex=5)
lines(c(mean(x[(indc==1),1]),mean(x[(indc==2),1])),c(mean(x[(indc==1),2]),mean(x[(indc==2),2])),lwd=5)
```

---

# Average Linkage v Centroid

- Consider an example with one variable (although everything works with vectors too).<!--D-->
--

- Suppose we have the clusters $\mathcal{A}=\left\{0,2\right\}$ and $\mathcal{B}=\left\{3,5\right\}$<!--D-->
--

- Find the distance $\mathcal{A}$ and $\mathcal{B}$ using<!--D-->
--

  + Average Linkage
  + Centroid Method

---

# Average Linkage

- Must find distances between all pairs of observations<!--D-->
--

  + $D(a_1,b_1)=3$
  + $D(a_1,b_2)=5$
  + $D(a_2,b_1)=1$
  + $D(a_2,b_2)=3$<!--D-->
--

- Averaging these, the distance is 3.

---

# Centroid method

- First find centroids<!--D-->
--

  + $\bar{a}=1$
  + $\bar{b}=4$<!--D-->
--

- The distance is 3.<!--D-->
--

- Here both methods give the same answer but when vectors are used instead they do not give the same answer in general.


---

# Average Linkage v Centroid

- In average linkage<!--D-->
--

  1. Compute the distances between pairs of observations
  2. Average these distances<!--D-->
--

- In the centroid method<!--D-->
--

  1. Average the observations to obtain the centroid of each cluster.
  2. Find the distance between centroids

---

# Ward's method

- All methods so far, merge two clusters when the distance between them is small.<!--D-->
--

- Wardâ€™s method merges two clusters to minimise within cluster variance.<!--D-->

---

# Within Cluster Variance

- The within-cluster variance for a cluster $\mathcal{A}$ is defined as

$$\mbox{V}_{\mbox{w}}(\mathcal{A})=\frac{1}{|\mathcal{A}|-1}S(\mathcal{A})$$

where 
$$S(\mathcal{A})=\sum_{\mathbf{a}_i\in\mathcal{A}}\left[\left(\mathbf{a}_i-{\mathbf{\bar{a}}}\right)'\left(\mathbf{a}_i-{\mathbf{\bar{a}}}\right)\right]$$

---

# Vector notation

- The term $S(\mathcal{A})=\sum\limits_{\mathbf{a}_i\in\mathcal{A}}\left(\mathbf{a}_i-{\mathbf{\bar{a}}}\right)'\left(\mathbf{a}_i-{\mathbf{\bar{a}}}\right)$ uses vector notation, but the idea is simple.
--

- Take the difference of each attribute from its mean (e.g. income, age, etc.)
--

- Then square them and add together over attributes **and** observations.
--

- The within cluster variance is a total variance across all attributes.

---

# Ward's algorithm

- At each step we must merge two clusters to form a single cluster.
--

- Suppose we pick a cluster $\mathcal{A}$ and $\mathcal{B}$ to form a new cluster $\mathcal{C}$.
--

- Ward's algorithm chooses $\mathcal{A}$ and $\mathcal{B}$ so that $V_{W}(\mathcal{C})$ is as small as possible.


---

class: middle, center, inverse

# Wrap-up

---

# Conclusions

- We have covered hierarchical clustering
- In BUSS6002 you will also cover *k-means clustering*.
- An advantage of hierarchical clustering is visualisation via the dendrogram.
- However the ideas of understanding when observations are similar, is useful in many other areas of business analytics.

---

class: middle, center, inverse

# Questions
